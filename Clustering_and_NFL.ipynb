{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Clustering and NFL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marlin97/DS-Unit-1-Sprint-4-Linear-Algebra/blob/master/Clustering_and_NFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9bK8maEkopO",
        "colab_type": "text"
      },
      "source": [
        "# PCA Notes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JHDhIiPks0z",
        "colab_type": "text"
      },
      "source": [
        "## Principal Components are not a re-labeling of the original features\n",
        "\n",
        "I saw some confusion yesterday about what the new Principal Components are that come out of our PCA transformations. Principal Components are a linear combination of any and all dimensions (features) that will increase their variance, this means that PCs are made up of a mixture of features --mostly the ones with the highest variance, but also smaller parts from other features. This means that they are not comparable to the original features of our $X$ matrix. In cases where we're not reducing dimensionality that much (like the Iris dataset) our Principal Components might be extremely similar to the original features (since there's not that many to pull from) but don't think of them in that way, think of them as a completely new dataset that we can't really apply \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CPPjEHVktro",
        "colab_type": "text"
      },
      "source": [
        "## PCA does not make predictions\n",
        "\n",
        "I would not call PCA a \"machine learning algorithm\" in that it does not try to make any predictions. We can't calculate any accuracy measure. You can call it an algorithm, you can call it a preprocessing technique or method, but it's not truly making predictions. This may have been confusing due to the fact that the Iris dataset had labels, but PCA is just re-organizing points in space, it's not making any predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK6eHmtDktur",
        "colab_type": "text"
      },
      "source": [
        "## PCA doesn't standardize the data for you\n",
        "\n",
        "You'll notice in the \"from scratch\" implementation of PCA that I did in class yesterday that in that example I did not divide the points by the standard deviation. I believe you'll get a slightly different set of points if you choose to divide by the standard deviation (I think this might be what A Apte was seeing yesterday when he tried both methods and found that they looked different. It could be something else entirely, but that's my first guess at what could be going on.)\n",
        "\n",
        "The Sklearn implementation does not standardize the points for you as part of the process. You can either do this yourself \"by hand\" or you can use other sklearn methods like this preprocessing step which will automatically standardize your data to have a mean of 0 and a standard deviation of 1. You have to do this **before** you pass your data to PCA.\n",
        "\n",
        "<https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwfQC5Yxksx1",
        "colab_type": "text"
      },
      "source": [
        "## PCA does not retain 100% of the information of the original dataset. \n",
        "\n",
        "Each component explains a certain % of the variance of the original dataset. PCA tries to maximize that variance, but you might need to use more than 2 components. \n",
        "\n",
        "Typically you want to use enough components in your analysis to keep the explained variance > 90%.\n",
        "\n",
        "So we're trading off losing a small-medium amount of predictive power for a reduction in dimensions/size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi9ks-kq_j8J",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro to Scree Plots\n",
        "\n",
        "A scree plot is a line plot of the eigenvalues of factors or principal components in an analysis. The scree plot is used to determine the number of factors to retain in an exploratory factor analysis (FA) or principal components to keep in a principal component analysis (PCA).\n",
        "\n",
        "A scree plot displays the eigenvalues in a downward curve, ordering the eigenvalues from largest to smallest. According to the scree test, the \"elbow\" of the graph where the eigenvalues seem to level off is found and factors or components to the left of this point should be retained as significant.\n",
        "\n",
        "![Scree Plot](https://upload.wikimedia.org/wikipedia/commons/a/ac/Screeplotr.png)\n",
        "\n",
        "![Variance Explained](https://www.analyticsvidhya.com/wp-content/uploads/2016/03/8-1-e1458532011651.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBXSEvwbODsA",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning (Overview)\n",
        "\n",
        "How do you know what kind of Machine Learning that you're doing? What algorithm should you pick? \n",
        "\n",
        "This decision is driven driven by:\n",
        "\n",
        "1) The attributes of your dataset\n",
        "\n",
        "2) What you want to predict\n",
        "\n",
        "![Types of Machine Learning](https://i.imgur.com/mZdJLdg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4qW9d7FlHUy",
        "colab_type": "text"
      },
      "source": [
        "- ## Supervised Learning: \n",
        "Supervised Learning is used when training data outputs are labelled. The output is the thing that you're trying to predict.\n",
        "  - ### Classification\n",
        "  Classification algorithms try to predict the correct category (or class) from a given set of categories.\n",
        "  - ### Regression\n",
        "  Regression algorithms predict a continuous or semi-continuous value. (Not to be confused with _Linear_ Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NynJ4Ye9lHg0",
        "colab_type": "text"
      },
      "source": [
        "-  ## Unsupervised Learning\n",
        "  - ### Clustering\n",
        "  Identifying groupings of related observations. This is our topic for today!\n",
        "  - ### Anomaly Detection\n",
        "  Identification of rare events or observations which raise suspicions by differing significantly from the majority of the data.\n",
        "  - ### Association Rule Learning\n",
        "  Association is a method of discovering relationships between observations in a dataset. (between ovservations or features, not just relationships between explanatory variables and a single output variable.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARo_wctGlHpT",
        "colab_type": "text"
      },
      "source": [
        "- ## Reinforcement Learning\n",
        "  A form of machine learning where an \"agent\" interacts with its environment and is rewarded for correct behavior and penalized for incorrect behavior. Over many iterations the agent learns the behavior that results in the greatest reward and smallest punishment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nPOjACaubCv",
        "colab_type": "text"
      },
      "source": [
        "##Memorize This!\n",
        "\n",
        "**Supervised**: Labelled outputs\n",
        "- **Classification**: Discrete output cagetories\n",
        "- **Regression**: Continuous output values\n",
        "\n",
        "**Unsupervised**: Outputs are not labelled\n",
        "\n",
        "**Reinforcement**: Rewards/punishments for \"behaviors\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9YeIVBQoAJR",
        "colab_type": "text"
      },
      "source": [
        "# Examples\n",
        "\n",
        "## [Classification Examples](https://github.com/ShuaiW/kaggle-classification)\n",
        "\n",
        " - Think Titanic Dataset\n",
        "\n",
        "## [Regression Examples](https://github.com/ShuaiW/kaggle-classification)\n",
        "\n",
        "- Think Home Price Prediction\n",
        "\n",
        "## [Unsupervised Learning Examples](http://www.lsi.upc.edu/~bejar/apren/docum/trans/09-clusterej-eng.pdf)\n",
        "\n",
        "- Think Iris Dataset (clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzdWvN9iugsd",
        "colab_type": "text"
      },
      "source": [
        "# ML Cheat Sheets\n",
        "\n",
        "![Microsoft Cheat Sheet](https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-cheat-sheet/machine-learning-algorithm-cheat-sheet-small_v_0_6-01.png)\n",
        "\n",
        "![PerceptionBox Cheat Sheet](https://i.pinimg.com/originals/a2/94/c1/a294c10effecc494735850ccb8c8ba16.png)\n",
        "\n",
        "This one does not group them by supervised, unsupervised, regression, classification, etc. But it gives you an idea of the different families of algorithms.\n",
        "\n",
        "![Algorithm Map](https://jixta.files.wordpress.com/2015/11/machinelearningalgorithms.png?w=816&h=521&zoom=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjCxoJFNl0ly",
        "colab_type": "text"
      },
      "source": [
        "# Clustering \n",
        "\n",
        "Clustering falls into the category of unsupervised learning. This is because there is nothing in our training data that designates the correct cluster that a data point should belong to beforehand. In fact, there's not even a \"correct\" _**number**_ of clusters to assign our points to. We will discuss some heuristics for choosing an **appropriate** number of clusters, but this (as in much of data science) is an area where there is no cut and dry right and wrong answer. \n",
        "\n",
        "Remember: \"All models are wrong, but some models are useful.\" Data science is all about acknowledging where your model might be wrong while still pursuing something useful. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6S6TOtgl0sb",
        "colab_type": "text"
      },
      "source": [
        "## Why Clustering?\n",
        "\n",
        "Clustering answers questions about how similar or dissimilar our \"data objects\" are. Clustering is one of the most effective methods for summarizing datasets with this question in mind. Clustering can be thought of as a sort of \"unsupervised classification.\" You will likely never deploy a clustering model to a production environment, they're too unreliable. Clustering is more useful as a tool for data exploration than a model for making predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDspwZ9Tl0yG",
        "colab_type": "text"
      },
      "source": [
        "## â€œClustering isnâ€™t hardâ€”itâ€™s either easy, or not interestingâ€\n",
        "\n",
        "If a good clustering exists, then it usually can be efficiently found. Clustering is the most difficult when clear clusters don't exist in the first place. In that case you should question whether or not clustering is the most appropriate or useful method. \n",
        "\n",
        "The purpose of clustering is to group data points that are similar along certain specified dimensions (attributes). \"Similarity\" is defined as the points being close together in some n-dimensional space. \n",
        "\n",
        "The greater the number of dimensions, the more difficult clustering becomes because the increase in dimensions makes all points this is because measures of distance are used to determine similarity between datapoints, and the greater the dimensionality the more all points become roughly equidistant with one another. (We don't have time to go further into this or demonstrate this, but clustering suffers from performance and interpretability issues in a high number of dimensions). Some of these challenges can be rectified by choosing an appropriate measure of \"distance\" between data points. For example, using clustering for document analysis is still fairly effective even though the analysis is of a highly-dimenaional space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldJ0c24yl02e",
        "colab_type": "text"
      },
      "source": [
        "# Types of Clustering:\n",
        "\n",
        "## Hierarchical:\n",
        "\n",
        "  - Agglomerative: start with individual points and combine them into larger and larger clusters\n",
        "  \n",
        "  - Divisive: Start with one cluster and divide the points into smaller clusters.\n",
        "\n",
        "## Point Assignment:\n",
        "\n",
        "  - We decide on a number of clusters out of the gate, and assign points to that number of clusters.\n",
        "\n",
        "# Hard vs Soft Clustering\n",
        "\n",
        "Hard Clustering assigns a point to a cluster\n",
        "\n",
        "Soft Clustering assigns each point a probability that it's in a given cluster.\n",
        "\n",
        "We're going to only deal with hard clustering, it's the more traditional approach. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnAcqxeDl06U",
        "colab_type": "text"
      },
      "source": [
        "## Applications:\n",
        "\n",
        "Astronomy: There's too much data from space for us to look at each individual start and galaxy and categorize it, but we can cluster them intro groups based on their observable attributes. \n",
        "\n",
        "[SkyCat](http://www.eso.org/sci/observing/tools/skycat.html)\n",
        "\n",
        "[Sloan Digital Sky Survey](https://www.sdss.org/)\n",
        "\n",
        "Document Classification / Grouping - We'll need to study a little bit of NLP before we can get into this. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhmN7D0Ol0-E",
        "colab_type": "text"
      },
      "source": [
        "## Distance Measures\n",
        "\n",
        "Did you know that there are distance measures other than euclidean distance?\n",
        "\n",
        "- Euclidean\n",
        "- Cosine\n",
        "- Jaccard\n",
        "- Edit Distance\n",
        "- Etc. \n",
        "\n",
        "Clustering traditionally uses Euclidean Distance, but this particular measure of distance breaks down in high dimensionality spaces. It's what we'll use for today. If you **LOVE**  clustering and want to put a strong focus on this area of Machine learning (at the expense of focusing strongly on others) then I would suggest further personal research into different clustering algorithms and distance measures. \n",
        "\n",
        "I want to reiterate that you don't have to use PCA and clustering in conjunction with each other. I think it's more common that they are not used together, but it can be useful in certain cases. We might try it today for fun and so reiterate how PCA is the preprocessing step, and K-means will be the main \"Machine Learning Algorithm.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY95oSIT-5ko",
        "colab_type": "text"
      },
      "source": [
        "## There are a lot of clustering algorithms. \n",
        "\n",
        "YOU DON'T NEED TO BE ABLE TO CODE ALL OF THEM FROM SCRATCH IN ORDER TO APPLY THEM OR EVEN TO UNDERSTAND THEM. FOCUS ON LEARNING THINGS WITHIN THE CONTEXT OF A PROBLEM YOU ARE TRYING TO SOLVE AND ONLY LEARN THOSE THINGS THAT WILL HELP YOU SOLVE THE PROBLEM. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJx_PxNUmyDT",
        "colab_type": "text"
      },
      "source": [
        "# K-Means Clustering\n",
        "\n",
        "![K-means Clustering](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/440px-K-means_convergence.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGSeEtls_QXU",
        "colab_type": "text"
      },
      "source": [
        "## The Process:\n",
        "\n",
        "Given a set of points in n-dimensional space we want to :\n",
        "\n",
        "1. We first select a number of groups to use and randomly initialize their respective center points. (To figure out the number of classes to use, itâ€™s good to take a quick look at the data and try to identify any distinct groupings.) \n",
        "2. Each data point is classified by computing the distance between that point and each group center, and then classifying the point to be in the group whose center is closest to it.\n",
        "3. Based on these grouped data points, we recompute the group center by taking the mean of all the vectors in the group.\n",
        "4. Repeat steps 2 & 3 for a set number of iterations or until the group centers donâ€™t change much between iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwzM7vnoIDx",
        "colab_type": "text"
      },
      "source": [
        "## Lets make some blobs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt03ADtDByNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "\n",
        "# Create dataframe of x, y and label values\n",
        "X, y = make_blobs(n_samples=100, centers=3, n_features=2)\n",
        "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
        "\n",
        "# Create scatterplot with coloring based on assigned labels\n",
        "colors = {0:'red', 1:'blue', 2:'green'}\n",
        "fig, ax = plt.subplots()\n",
        "grouped = df.groupby('label')\n",
        "for key, group in grouped:\n",
        "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
        "plt.show()                  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqYOxqPP-koM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3fV8Cw_Eq6w",
        "colab_type": "text"
      },
      "source": [
        "## Linear Separability\n",
        "The 2D blobs below are what is called \"linearly separable\" Meaning that we could use straight lines to separate them with no errors. This is the most trivial case of of k-means clustering, but it will help us to demonstrate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK624RjW-qGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop labels to prove that this is truly unsupervised learning\n",
        "points = df.drop('label', axis=1)\n",
        "points.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro4aO_mPoP1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scatter plot of our label-less data\n",
        "plt.scatter(points.x, points.y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzETDJC2FNDK",
        "colab_type": "text"
      },
      "source": [
        "## Calculating the Centroid\n",
        "\n",
        "K-means clustering is what's known as a centroid-based clustering algorithm. A centroid is an imaginary point located at the average location of all of the points in a given cluster. For example, if I wanted to find the centroid of all of the points in the above graph I would just calculate the average of the dataset's x-coordinates to find the x value of the centroid, and the average of the dataset's y-coordinates to find the y value of the centroid.\n",
        "\n",
        "If we plot the centroid on the graph you'll see that it lies in the middle of the points. You could imagine the centroid as if it is the center of gravity, or center of mass for a given cluster. Since in this example we're treating all of the points in the dataset as if they're in the same cluster, it will end up somewhere in the middle. We're just doing this to demonstrate what a centroid is. The K-means algorithm doesn't ever calculate the centroid for the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLZyrzOE_CnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the centroid of the entire dataset (only for demonstration purposes)\n",
        "dataset_centroid_x = points.x.mean()\n",
        "dataset_centroid_y = points.y.mean()\n",
        "\n",
        "print(dataset_centroid_x, dataset_centroid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OgBWw0c_cVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax.plot(points.x, points.y)\n",
        "ax = plt.subplot(1,1,1)\n",
        "ax.scatter(points.x, points.y)\n",
        "ax.plot(dataset_centroid_x, dataset_centroid_y, \"or\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR5sjoKhm5VS",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the algorithm by choosing random points to serve as the initial fake \"centroids\"\n",
        "\n",
        "These will get updated to become real cluster centroids after the first iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvuDTEOM_4Ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centroids = points.sample(3)\n",
        "centroids.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pw7xVK9nprq",
        "colab_type": "text"
      },
      "source": [
        "# Plot initial \"fake\" centroids on the graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXz0-YRuAGIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.subplot(1,1,1)\n",
        "ax.scatter(points.x, points.y)\n",
        "ax.plot(centroids.iloc[0].x, centroids.iloc[0].y, \"or\")\n",
        "ax.plot(centroids.iloc[1].x, centroids.iloc[1].y, \"og\")\n",
        "ax.plot(centroids.iloc[2].x, centroids.iloc[2].y, \"oy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmVwnSM1R6tz",
        "colab_type": "text"
      },
      "source": [
        "# Re-review steps of the algorithm\n",
        "\n",
        "Given a set of points in n-dimensional space we want to :\n",
        "\n",
        "1. We first select a number of groups to use and randomly initialize their respective center points. (To figure out the number of classes to use, itâ€™s good to take a quick look at the data and try to identify any distinct groupings.) \n",
        "2. Each data point is classified by computing the distance between that point and each group center, and then classifying the point to be in the group whose center is closest to it.\n",
        "3. Based on these grouped data points, we recompute the group center by taking the mean of all the vectors in the group.\n",
        "4. Repeat steps 2 & 3 for a set number of iterations or until the group centers donâ€™t change much between iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9LlmLSySTb7",
        "colab_type": "text"
      },
      "source": [
        "## 3-means clustering\n",
        "\n",
        "Lets pick k=3 and start demonstrating how this algorithm actually works. \n",
        "\n",
        "The k-means algorithm works by picking 3 of the actual datapoints at random (in the simplest case) and treating those as the starting centroids. Using those centroids, 3 clusters are calculated.\n",
        "\n",
        "We then use the new clusters and calculate a new centroid for each of them. Then, using those centroids we re-cluster. We perform this process over and over again until our clusters stabilize and the centroids stop moving. Lets demonstrate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nbwx8_pA3DU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Calculate the Nearest Centroid to each data point\n",
        "def find_nearest_centroid(df, centroids, iteration):\n",
        " \n",
        "  # calculate the distances between each point and each centroid\n",
        "  distances = distance.cdist(df, centroids, 'euclidean')\n",
        "  \n",
        "  # Get nearest centroid to each point based on distance\n",
        "  nearest_centroids = np.argmin(distances, axis=1)\n",
        "\n",
        "  se = pd.Series(nearest_centroids)\n",
        "  df['cluster_'+iteration] = se.values\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACmU-SzWoJ6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_pass = find_nearest_centroid(points.select_dtypes(exclude='int64'), centroids, '1')\n",
        "first_pass.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVZQLvfAFh60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_clusters(df, column_header, centroids):\n",
        "  colors = {0:'red', 1:'green', 2:'yellow'}\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(centroids.iloc[0].x, centroids.iloc[0].y, \"ok\")\n",
        "  ax.plot(centroids.iloc[1].x, centroids.iloc[1].y, \"ok\")\n",
        "  ax.plot(centroids.iloc[2].x, centroids.iloc[2].y, \"ok\")\n",
        "  grouped = df.groupby(column_header)\n",
        "  for key, group in grouped:\n",
        "      group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
        "  plt.show()\n",
        "  \n",
        "plot_clusters(first_pass, 'cluster_1', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_6cOfkeFwWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_centroids(df, column_header):\n",
        "  new_centroids = df.groupby(column_header).mean()\n",
        "  return new_centroids\n",
        "\n",
        "centroids = get_centroids(first_pass, 'cluster_1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyZLdP_nHKBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "centroids = get_centroids(first_pass, 'cluster_1')\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "second_pass = find_nearest_centroid(first_pass.select_dtypes(exclude='int64'), centroids, '2')\n",
        "\n",
        "# Plot New Cluster\n",
        "plot_clusters(second_pass, 'cluster_2', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBueXP01G8tE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "centroids = get_centroids(second_pass, 'cluster_2')\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "third_pass = find_nearest_centroid(second_pass.select_dtypes(exclude='int64'), centroids, '3')\n",
        "\n",
        "# Plot New Cluster\n",
        "plot_clusters(third_pass, 'cluster_3', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u6KiGdvpD-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "centroids = get_centroids(third_pass, 'cluster_3')\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "fourth_pass = find_nearest_centroid(third_pass.select_dtypes(exclude='int64'), centroids, '4')\n",
        "\n",
        "# Plot New Cluster\n",
        "plot_clusters(fourth_pass, 'cluster_4', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaTbGpAcpPSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "centroids = get_centroids(fourth_pass, 'cluster_4')\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "fifth_pass = find_nearest_centroid(fourth_pass.select_dtypes(exclude='int64'), centroids, '5')\n",
        "\n",
        "# Plot New Cluster\n",
        "plot_clusters(fifth_pass, 'cluster_5', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXpje6GWpbYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate New Centroids\n",
        "centroids = get_centroids(fifth_pass, 'cluster_5')\n",
        "\n",
        "# Get Clusters for New Centroids\n",
        "sixth_pass = find_nearest_centroid(fifth_pass.select_dtypes(exclude='int64'), centroids, '6')\n",
        "\n",
        "# Plot New Cluster\n",
        "plot_clusters(sixth_pass, 'cluster_6', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKo9xSypJvPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convergence = np.array_equal(fifth_pass['cluster_5'], sixth_pass['cluster_6'])\n",
        "\n",
        "print(\"Are we at convergence?? - \", convergence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njKZU3ZVqELY",
        "colab_type": "text"
      },
      "source": [
        "## Here's some better code that doesn't rely upon renaming dataframes to create new plots and demonstrate the steps in the k-means process (courtesy of the python wizard Zach Angell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS9z6osaGqZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_nearest_centroid(df, centroids):\n",
        "  last_centroids = [np.random.choice([0,1,2])] * df.shape[0]\n",
        "  df_temp = df.copy()\n",
        "  i = 0\n",
        "  \n",
        "  while True:\n",
        "    if i>0:\n",
        "      centroids = get_centroids(df_temp, 'cluster' + str(i-1))\n",
        "\n",
        "    distances = cdist(df_temp[['x', 'y']], centroids[['x', 'y']])\n",
        "    nearest_centroids = np.argmin(distances, axis=1)\n",
        "    \n",
        "    df_temp['cluster' + str(i)] = nearest_centroids\n",
        "    \n",
        "    if (list(nearest_centroids) == list(last_centroids)):\n",
        "      return df_temp\n",
        "    else:\n",
        "      i +=1\n",
        "      last_centroids = nearest_centroids\n",
        "      \n",
        "def get_centroids(df, column_header):\n",
        "  x = [df.x[df[column_header]==0].mean(), df.x[df[column_header]==1].mean(), df.x[df[column_header]==2].mean()]\n",
        "  y = [df.y[df[column_header]==0].mean(), df.y[df[column_header]==1].mean(), df.y[df[column_header]==2].mean()]\n",
        "  data = {'x': x, 'y' : y}\n",
        "  return pd.DataFrame(data)\n",
        "  \n",
        "  \n",
        "find_nearest_centroid(points, centroids).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-nJ_i-NQZIM",
        "colab_type": "text"
      },
      "source": [
        "## How many centroids == K-means \n",
        "\n",
        "Since the centroid is the mean of a cluster the number of centroids to choose is the most important decision to make in \"k-means\" clustering. The K value is the number of centroids.\n",
        "\n",
        "\n",
        "### The Eyeball Method\n",
        "/polly \"How many centroids (means) should we use for this exercise?\"\n",
        "\n",
        "Congratulations, you've just been introduced to the first method of _**picking k**_ - Just graph your points and pick a number that makes sense. This gets a lot harder once you get a dimensionality higher than 3, but... Didn't we learn about some way to take high dimensional data and turn it into 2 or 3 dimensions...? ðŸ˜€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGueCso5SFXN",
        "colab_type": "text"
      },
      "source": [
        "## Lets use a library to do it: Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmyHklDKSI_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans \n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gj9sXFDWtLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add our new labels to the dataframe\n",
        "new_series = pd.Series(labels)\n",
        "df['clusters'] = new_series.values\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on8qYidhXaA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We might get unlucky with different centroids and have slightly different clusters.\n",
        "centroids = get_centroids(df, 'clusters')\n",
        "plot_clusters(df, 'clusters', centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF5d6AP_VT4c",
        "colab_type": "text"
      },
      "source": [
        "# Important Considerations:\n",
        "\n",
        "## Choosing the appropriate clustering method \n",
        "\n",
        "We've only taught you one so stick with that for today. \n",
        "\n",
        "## Choosing appropriate dimensions to cluster along. \n",
        "\n",
        "Hmmm, what would be the best dimension to cluster along? Maybe one that helps separate the clusters the best. You can do a lot of scatterplots to examine this or you could, I dunno, use a technique that maximizes the variance along certain dimensions transforming the data into principal components and then cluster along the dimensions of the principal components. \n",
        "\n",
        "## Choosing a distance measure\n",
        "\n",
        "Euclidean is the most traditional, you'll learn the others if the occasion presents itself (it most likely won't) - If I'm being completely honest.\n",
        "\n",
        "## Choosing an appropriate k (# of clusters)\n",
        "\n",
        "THIS IS THE MOST IMPORTANT CONSIDERATION WHEN IT COMES TO K-MEANS (I mean it's in the name)\n",
        "\n",
        "![Elbow Method](https://www.datanovia.com/en/wp-content/uploads/dn-tutorials/004-cluster-validation/figures/015-determining-the-optimal-number-of-clusters-k-means-optimal-clusters-wss-silhouette-1.png)\n",
        "\n",
        "On the x-axis we have number of centroids (k)\n",
        "\n",
        "On the y-axis we have \"distortion\" which is measured as the sum of squared distances of each point to its given cluster\n",
        "\n",
        "Here's some code below that could be used to create a similar \"Elbow\" Graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXflkYwTL8WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(points)\n",
        "    sum_of_squared_distances.append(km.inertia_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxrwEX4VL-u5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhMkzbIFX96q",
        "colab_type": "text"
      },
      "source": [
        "# Further Considerations\n",
        "\n",
        "## Choosing an appropriate K\n",
        "\n",
        "## Unlucky Initial Centroids\n",
        "\n",
        "Unlucky Initial Centroids can \n",
        "\n",
        "- result in a poor clustering\n",
        "- lead to a clustering that doesn't converge\n",
        "\n",
        "## Computational Complexity\n",
        "\n",
        "## What is K-means good for?\n",
        "\n",
        "- ### Mostly Round, linearly-separable blobs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEEQapU2uRIx",
        "colab_type": "text"
      },
      "source": [
        "# No Free Lunch\n",
        "\n",
        "The no free lunch principle states that the more an algorithm is optimized to solve one specific kind of problem, the worse it gets at solving all other kinds of problems. \n",
        "\n",
        "This means that if you want an algorithm that's really good at solving a certain problem (cluster shape for example), it usually lose some of its ability to generalize to other problems. \n",
        "\n",
        "### What does this mean for us as data scientists?\n",
        "\n",
        "1) There are always tradeoffs when selecting from different approaches. Because of this, understanding those tradeoffs and justifying your choice of methodology is just as important as actually doing the work itself.\n",
        "\n",
        "2) The only way that we can choose one approach over another is to make assumptions about our data. If we don't know anything about the characteristics of our data, then we can't make an informed choice of algorithm. \n",
        "\n",
        "Think about how we knew to use Unsupervised vs Supervised learning for the clustering problem, the choice was informed by our data. Does it have labels or not? \n",
        "\n",
        "![No Free Lunch](https://cdn-images-1.medium.com/max/1600/1*oNt9G9UpVhtyFLDBwEMf8Q.png)\n",
        "\n",
        "Density Based Clustering Animation:\n",
        "\n",
        "[DB Scan Animation](https://www.youtube.com/watch?v=h53WMIImUuc)\n",
        "\n",
        "## Don't Get Overwhelmed! \n",
        "\n",
        "Some people spend their entire careers researching new clustering methods and improvements.\n",
        "\n",
        "## Don't be a perfectionist! \n",
        "\n",
        "There are too many techniques to master, you can't learn all of them in 7 months.\n",
        "\n",
        "## Focus on learning in the context of a problem you want to solve or a project that you are passionate about building\n"
      ]
    }
  ]
}